# GET aHEAD
Burpsuite is nice. Use default settings. Once it boots up, go to proxy and open browser from the Burpsuite app. Then go to the website I want to exploit. In this case it’s http://mercury.picoctf.net:21939/  . Mess around with the website a few times. Go to HTTP history and see the GET and POST history. You can manipulate this. (side note, keeping intercept mode off would be nice so you don’t get a popup from Burp constantly while going through webpages). Right click on the Raw Post request, and send to repeater. DO NOT COPY AND PASTE, BURP DOES NOT LIKE THAT. IT’LL ACT LIKE NOTHING WORKS. Change the POST to HEAD and check out the response. bam. The flag is there. You can copu

# Cookies
First of all, I need to download Chrome to my linux finally. 
sudo dpkg -i google-chrome-stable_current_amd64.deb
	//after I download the Debian file from chrome ofcourse.
bam
Now go to the website pico provides. It says to type in “snickerdoodle” in the search bar. Inspect, go to application, check out cookies. The value for the one cookie working in the page is 0. That value can be edited. If you change the cookie value and refresh the page, it will change. Imagine this as different sub-domains. The flag is hiding in a certain valued webpage. Keep changing the number value to find the correct page. 

# Insp3ct0r
Go to the website and inspect the page. The flag is divided into three. Look through the code. 

# Bookmarklet
According to wikipedia, a bookmarklet is a piece of code that can be used as a bookmark to add new browser features. Follow the link and it'll send you to a page with bookmarklet to copy. Tried using Visual Studio Code but no luck but you can clearly see that the flag is encoded. Created a new bookmark and pasted the code into the URL. Open that "URL" and it popped a text box with the flag. 

# Scavenger hunt
Inspect webpage as normal. Look at sources. There’s parts of the flag through out the code but some are still missing. The js has a comment asking how can it keep Google from indexing its website. That’s /robots.txt. Go over there and another comment suggests it’s on an Apache server and if we can get the other flag piece. So now we need to learn what an .htaccess is but more on that soon. Replace robots.txt with .htaccess so the URL should look like http://mercury.picoctf.net:44070/.htaccess  . Another coment says it can store a lot of information when it makes websites with Mac. Replace .htaccess with .DS_Store and we find the final piece. 
.htaccess is specifically used for Apache web servers to execute code and govern the entire website or specific directories. It’s short for hypertext access and is a distributed server configuration file. The code is read and execute from top to bottom. Important to know because it can accidently break a website if configured improperly. 
.DS_Store is short for Desktop Services Store and is an invisible file on macOS. It’s created when you look into any folder with ‘Finder’ and stores data/meta data, even if the .DS_Store is compressed into a zip. This is a vulnerability and should be prevented by turning off automatic creation of these files.
Press command + spacebar and search for Terminal.
Open Terminal. Then copy/paste this command and press enter:
defaults write com.apple.desktopservices DSDontWriteNetworkStores true
To reverse this if ever needed in the future, just change true to false:
defaults write com.apple.desktopservices DSDontWriteNetworkStores false
After running one of these commands, reboot your machine.
You’re set up & secure!
https://buildthis.com/ds_store-files-and-why-you-should-know-about-them/

# WebDecode
You can shift through the home, about, and contact page. However, the about page says you might find the flag if youn inspect that one. Inspect the element and you'll see the HTML section right before <h1> header stating to inspect the flag. The second has an odd string that 'notify_true=". It looks base64 and you can ask ChatGPT to confirm. 
    https://www.base64decode.org/
Use the website and decode. It's only encoded by base64 once. Important to note for later I'm sure as other base64's may be encoded multiple times.

# where are the robots
Follow the link provided. Aside from the title, the hint gives it away too. /robots.txt tells search engine crawlers where they can/cannot access. Add robots.txt to the file path and it'll say what URL is disallowed. Copy that part of the URL and replace robots.txt in the search bar. Boom.

# Logon
Pay attention to those cookies. When you try to log on without a password, you see an Admin cookie if you inspect and go to application. Edit that cookie value to True. Refresh. 

# dont-use-client-side
Inspect the page. See Sources. Just pay attention to the splits and go in chronological order. 

# It is my Birthday
Learned a bit more about MD5 collisions. There's a forum that discusses this and provided multiple files with the same hash despite it being completely different. Saved those files, changed them to a pdf through the terminal. //mv filename1.bin filename1.pdf// Uploaded the documents in the provided website from pico and it produced the python scrypt with the flag in it.


![image](https://github.com/ChaseDepth/PicoNotes/assets/156110306/37c02317-d9aa-4265-bc18-cc1046fdca44)


